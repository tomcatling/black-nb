{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing\n",
    "## 1. Data Representation\n",
    "### ASI Data Science Fellowship XIII - 24th January 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data used in this notebook\n",
    "!wget \"https://s3-eu-west-1.amazonaws.com/fellowship-teaching-materials/NLP/tweets.pkl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will take you through many of the concepts we have introduced in this session. We will use the same dataset for all examples, namely a collection of 6000 or so tweets from @realDonaldTrump and @BarackObama. \n",
    "\n",
    "Wherever possible we will use `sklearn`, Python's machine learning library that you are most likely already familiar with. For a few tasks we will turn to `nltk` (natural language toolkit) a Python library for Nautural Language Procession (NLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_pickle('tweets.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many things to consider when cleaning text data. Some problems are common to other data types, such as how to deal with missing values. Others are unique to text data, and include things like removing HTML tags or urls. We don't want to focus too much on data cleaning for the purposes of this course, we've done a little bit of cleaning below to give you a taste. Generally speaking regular expressions (available in Python in the `re` module) will get you pretty far. For specific tasks there are often existing libraries you can use. For example `feedparser` is good for getting data from an RSS feed, `beautifulsoup` is good for parsing HTML/XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tweet(text):\n",
    "    # encode tweets as utf-8 strings\n",
    "    text = text.decode('utf-8')\n",
    "    # remove commas in numbers (else vectorizer will split on them)\n",
    "    text = re.sub(r',([0-9])', '\\\\1', text)\n",
    "    # sort out HMTL formatting of &\n",
    "    text = re.sub(r'&amp', 'and', text)\n",
    "    # strip urls\n",
    "    return re.sub(r'http[s]{0,1}://[^\\s]*', '', text)\n",
    "\n",
    "df['text'] = df['text'].map(clean_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field of NLP contains a lot of jargon from linguistics. We don't want to get too bogged down in defining lots of new terms, but the following two are helpful:\n",
    "\n",
    "- Type: An element of the vocabulary. May be a word, may be an n-gram (ordered sequence of words)\n",
    "- Token: An instance of a type in running text.\n",
    "\n",
    "Any given language has a large enough vocabulary that trying to do data science on the set of all possible sentences is totally impractical. Instead it helps to break text up into smaller chunks, a process called tokenizing.\n",
    "\n",
    "Exactly how we do this will depend on the problem, but some common ways include splitting on whitespace, or splitting on non-alphanumeric characters. In general, the method of tokenizing will be informed by the format of the text data being studied.\n",
    "\n",
    "**Tokenizers are accessed in a slightly roundabout way in `sklearn`, as below. Run this cell a few times to tokenize random tweets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from random import randint\n",
    "\n",
    "# tokenize a random tweet\n",
    "i = randint(0, len(df) - 1)\n",
    "tokenizer = CountVectorizer().build_tokenizer()\n",
    "tokenizer(df['text'].iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing breaks our raw text data down into more manageable chunks, but it's still not in a form that is particularly useful for training models. Let's look at a few common, simple ways of vectorizing text data. We will use `sklearn` which can efficiently vectorize text data and stores everything as `scipy` sparse arrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the simplest way to vectorize is to simply create a vector of counts of the number of times any type appears in a given piece of text.\n",
    "\n",
    "To get some intuition, let's try it on a small test corpus of 10 random tweets.\n",
    "\n",
    "**Use `sample` on the series `df['text']` to get a random selection of 10 tweets**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "test_corpus = # your code here\n",
    "\n",
    "test_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the sample, we can create count vectors using `CountVectorizer` from `sklearn`. We set `max_features=5` so as to work with a small vocabulary of only the most common terms.\n",
    "\n",
    "See the next cell for usage of `CountVectorizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a count vectorizer with our desired parameters\n",
    "count_vectorizer = CountVectorizer(max_features=5)\n",
    "\n",
    "# first 'fit' the vectorizer to the corpus\n",
    "# this step automatically determines the vocabulary\n",
    "count_vectorizer.fit(test_corpus)\n",
    "\n",
    "# then 'transform' the corpus to count vectors (a matrix)\n",
    "count_vectors = count_vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = count_vectorizer.get_feature_names()\n",
    "# we use .toarray() to convert from sparse \n",
    "# array to dense numpy array\n",
    "for i, row in enumerate(count_vectors.toarray()):\n",
    "    print(test_corpus.iloc[i])\n",
    "    print(pandas.DataFrame({'Terms': features, 'Counts': row}).to_string(index=False))\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term frequency vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count vectors are very sensitive to document length. In our case we expect all tweets to be similar lengths, but in general we might be dealing with documents of varying lengths, so it makes sense to normalise the count vectors. This results in so-called frequency vectors.\n",
    "\n",
    "**Using `TfidfVectorizer`, compute term frequency vectors for the test corpus and print them out as we did for the count vectors. Make sure you set `use_idf=False` when initialising your `TfidfVectorizer`. As before limit the vocabulary to 5 types.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_vectorizer = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "features = tf_vectorizer.get_feature_names()\n",
    "for i, row in enumerate(<your_vectors>):\n",
    "    # print out your vectors in a nice way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tfidf vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf-idf stands for 'term frequency - inverse document frequencys. Given our term frequencys, we re-weight by the inverse of the document frequency. Therefore a given term will have a larger value if it both appears many times in the document, but appears infrequently across the corpus. In this sense it automatically detects and upweights terms which are likely to be able to help us distinguish between documents.\n",
    "\n",
    "**Compute tfidf vectors for your test_corpus. You can once again use `TfidfVectorizer`, but this time set `use_idf=True`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "# print out your vectors in a nice way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "So far we have only considered individual words and their frequencies. We lose a lot of information doing so, because we discard word order and grammar etc.\n",
    "\n",
    "A simple solution to this is to use n-grams, that is sequences of words of length n, when we tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can tokenize/vectorize with n-grams using the parameter\n",
    "# ngram_range. It takes a tuple of ints that specify min and max\n",
    "# n-gram lengths\n",
    "ngram_vectorizer = CountVectorizer(max_features=5, ngram_range=(2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use `ngram_vectorizer` to compute bigram count vectors for your test corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "outputs": [],
   "source": [
    "# print your vectors in a nice way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same principles as before, namely vectorizing using term frequencies or term frequency-inverse document frequencies, apply here too.\n",
    "\n",
    "A big advantage of tokenizing using n-grams is that models can learn some basic information about which words tend to appear together, and which words follow on from other sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generative Model\n",
    "\n",
    "We've written a tweet generator that uses n-grams and a simple Markov model to generate new tweets based on some training data. You can see that when using unigrams, it just returns a random collection of words that follow the same distribution as the observed data. However bigrams and trigrams already manage to capture a lot of information about how words are used together.\n",
    "\n",
    "You can try the model using 10-grams or some large value of n too. But at that point there is not enough training data to make the Markov model particularly interesting. The model will just start to repeat actual tweets rather than generating new content. It will have massively overfit the data.\n",
    "\n",
    "If you have time, feel free to dive into the source code to see how the generator works, but you are also more than welcome to just use it as a black box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from asi_nlp.twitter import TweetGenerator\n",
    "\n",
    "unigram_generator = TweetGenerator(1)\n",
    "unigram_generator.train(df[df['label'] == 0]['text'])\n",
    "\n",
    "bigram_generator = TweetGenerator(2)\n",
    "bigram_generator.train(df[df['label'] == 0]['text'])\n",
    "\n",
    "trigram_generator = TweetGenerator(3)\n",
    "trigram_generator.train(df[df['label'] == 0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_generator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_generator.generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_generator.generate()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
